{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、数据加载和探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1、加载数据\n",
    "train_data = pd.read_csv('./train.csv')\n",
    "test_data = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2、数据探索\n",
    "# print(train_data.info())  # 直观看出每列数据的条数及类型，可以先简单确认数据缺失情况\n",
    "# print(\"=\" * 10)\n",
    "# print(train_data.describe())  # 查看所有连续型特征的基本信息，包括总条数，均值，标准差，最大值，最小值和四分位数\n",
    "# print(\"=\" * 10)\n",
    "# print(train_data.describe(include=[\"O\"]))  # 查看所有离散型变量的基本信息，包括总条数，取值个数，最多取值，以及最多取值个数\n",
    "# print(\"=\" * 10)\n",
    "# print(train_data.head())  # 查看前五条数据，感觉一下\n",
    "# print(\"=\" * 10)\n",
    "# print(train_data.tail())  # 查看后五条数据，感觉一下\n",
    "# print(\"=\" * 10)\n",
    "# print(train_data.columns) # 查看一下列名\n",
    "# print(\"=\" * 10)\n",
    "# print(train_data.select_dtypes(['float']).columns) # 查看一下float类型的列有哪些\n",
    "# print(\"=\" * 10)\n",
    "# print(train_data.select_dtypes(['object']).columns.values) # 查看一下object类型的列有哪些，这些列往往是必须数值化的列\n",
    "# print(\"=\" * 10)\n",
    "# print(train_data.select_dtypes(['int64']).columns) # 查看一下int64类型的列有哪些\n",
    "# print(\"=\" * 10)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Survived': 0, 'Age': 177, 'Sex': 0, 'Embarked': 2, 'SibSp': 0, 'Cabin': 687, 'Pclass': 0, 'Parch': 0, 'Ticket': 0, 'Fare': 0, 'PassengerId': 0, 'Name': 0}\n",
      "{'Parch': 0, 'Pclass': 0, 'Name': 0, 'Sex': 0, 'Cabin': 327, 'Embarked': 0, 'Ticket': 0, 'Fare': 1, 'SibSp': 0, 'PassengerId': 0, 'Age': 86}\n"
     ]
    }
   ],
   "source": [
    "# 1、分析数据缺失量\n",
    "def get_na_count(data_df):\n",
    "    column_list = data_df.columns\n",
    "    na_count_dict = dict()\n",
    "    for column in column_list:\n",
    "        na_count_dict.update({column: data_df[column].isna().sum()})\n",
    "    return na_count_dict\n",
    "\n",
    "na_count_dict = get_na_count(train_data)\n",
    "print(na_count_dict)\n",
    "print(get_na_count(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Survived': 'discrete', 'Age': 'continuous', 'Sex': 'discrete', 'Embarked': 'discrete', 'SibSp': 'discrete', 'Cabin': 'discrete', 'Pclass': 'discrete', 'Parch': 'discrete', 'Ticket': 'discrete', 'Fare': 'continuous', 'PassengerId': 'discrete', 'Name': 'discrete'}\n",
      "{'Age': 'continuous', 'Sex': 'discrete', 'Embarked': 'discrete', 'SibSp': 'discrete', 'Cabin': 'discrete', 'Pclass': 'discrete', 'Parch': 'discrete', 'Ticket': 'discrete', 'Fare': 'continuous', 'PassengerId': 'discrete', 'Name': 'discrete'}\n",
      "{'Survived': 0, 'Age': 0, 'Sex': 0, 'Embarked': 0, 'SibSp': 0, 'Cabin': 0, 'Pclass': 0, 'Parch': 0, 'Ticket': 0, 'Fare': 0, 'PassengerId': 0, 'Name': 0}\n",
      "{'Parch': 0, 'Pclass': 0, 'Name': 0, 'Sex': 0, 'Cabin': 0, 'Embarked': 0, 'Ticket': 0, 'Fare': 0, 'SibSp': 0, 'PassengerId': 0, 'Age': 0}\n"
     ]
    }
   ],
   "source": [
    "# 2、根据缺失量情况进行数据填充\n",
    "# 整行缺失数据进行删除，重复行数据删除，年龄不合理数据删除。\n",
    "# 部分缺失数据按类型填充，这里对连续型特征用均值填充，离散型特征值用众数填充\n",
    "from pandas.api.types import is_string_dtype, is_float_dtype\n",
    "def judge_variable_type(data_df):\n",
    "    # 这里我们通过特征变量类型来区分出连续型和离散型。主要是为了解决object类离散型变量。\n",
    "    # 也可以通过对特征取值个数做一个阈值限定，小于多少个，则认为是离散型，否则则认为是连续型。\n",
    "    column_list = data_df.columns\n",
    "    column_type_dict = dict()\n",
    "    for column in column_list:\n",
    "        if is_string_dtype(data_df[column]):\n",
    "            column_type_dict.update({column:\"discrete\"})  # object归到离散型变量\n",
    "        if is_float_dtype(data_df[column]):\n",
    "            column_type_dict.update({column:\"continuous\"})  # float归到连续型变量\n",
    "        else:\n",
    "            column_type_dict.update({column:\"discrete\"})  # 其他也归到离散型变量，这个看情况调整\n",
    "    return column_type_dict\n",
    "\n",
    "def fill_data(data_df, na_count_dict=None): \n",
    "    if na_count_dict is None:\n",
    "        na_count_dict = get_na_count(data_df)\n",
    "        \n",
    "    # 填充之前，先适当删除一些不需要的数据\n",
    "    data_df.dropna(how=\"all\", inplace=True)  # 删除全行都是空的\n",
    "    data_df.drop_duplicates(keep=\"first\", inplace=True) # 删除重复的行，只保留第一处\n",
    "    data_df.drop(data_df[(data_df[\"Age\"] < 0) | (data_df[\"Age\"] > 100)].index, inplace=True)  # 删除年龄中不合适的\n",
    "    data_df.reset_index(drop=True, inplace=True)  # 删除了数据，重置一下索引\n",
    "    \n",
    "    column_type_dict = judge_variable_type(data_df)\n",
    "    print(column_type_dict)\n",
    "    for column,na_count in na_count_dict.items():\n",
    "        if na_count == 0:\n",
    "            continue\n",
    "        if column_type_dict[column] == \"continuous\":\n",
    "            target_num = data_df[column].mean()  # 获得均值\n",
    "        else:\n",
    "            target_num = data_df[column].value_counts().head(1).index[0]  # 获得众数\n",
    "        data_df[column].fillna(target_num, inplace=True)\n",
    "\n",
    "fill_data(train_data)\n",
    "fill_data(test_data)\n",
    "print(get_na_count(train_data))\n",
    "print(get_na_count(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obj_column_list: ['Sex', 'Ticket', 'Cabin', 'Embarked']\n",
      "useless_columns: ['PassengerId', 'Name']\n",
      "useless column already removed\n"
     ]
    }
   ],
   "source": [
    "# 3、看看哪些数据要做数值化处理，同时找出信息量为0或信息量太大的特征列，进行删除操作\n",
    "def get_obj_column(data_df):\n",
    "    obj_column_list = list()\n",
    "    useless_columns = list()\n",
    "    data_length = data_df.shape[0]\n",
    "    column_list = data_df.columns\n",
    "    for column in column_list:\n",
    "        unique_count = len(data_df[column].unique())\n",
    "        if unique_count == 1 or unique_count == data_length:\n",
    "            useless_columns.append(column)  # 找到明显无效的特征列\n",
    "            continue\n",
    "        if isinstance(data_df[column][0], str):\n",
    "#             obj_column_dict.update({column:len(data_df[column].unique())})\n",
    "            obj_column_list.append(column)\n",
    "    return obj_column_list, useless_columns\n",
    "\n",
    "obj_column_list, useless_columns = get_obj_column(train_data)\n",
    "print(\"obj_column_list: {}\".format(obj_column_list))\n",
    "print(\"useless_columns: {}\".format(useless_columns))\n",
    "\n",
    "def remove_useless_data(data_df_train, data_df_test, useless_columns=None):\n",
    "    if useless_columns is None:\n",
    "        _, useless_columns = get_obj_column(data_df_train)\n",
    "    for column in useless_columns:\n",
    "        data_df_train.drop(column, axis=1, inplace=True)\n",
    "        data_df_test.drop(column, axis=1, inplace=True)\n",
    "\n",
    "remove_useless_data(train_data, test_data, useless_columns)\n",
    "print(\"useless column already removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obj_column_list:  ['Sex', 'Ticket', 'Cabin', 'Embarked']\n",
      "obj encode finished\n"
     ]
    }
   ],
   "source": [
    "# 4、对字符串特征进行数值化，这里提供两种方式：OneHotEncoder和DictVectorizer，但我们使用DictVectorizer\n",
    "# 离散无序变量，变量取值为整数：OneHotEncoder\n",
    "    # 当想要看到特征对应某个取值的重要性时，适合使用OneHot。\n",
    "    # 当特征数过多时，树模型不宜使用OneHot，防止树过深过拟合，参考LightGBM\n",
    "# 离散无序变量，变量取值为字符串：DictVectorizer\n",
    "    # 能自动处理新出现的特征取值，会默认归置为0。\n",
    "    \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def dict_encoder(data_df_train, data_df_test, obj_column_list=None):\n",
    "    if not obj_column_list:\n",
    "        obj_column_list, _ = get_obj_column(data_df_train)\n",
    "    print(\"obj_column_list: \", obj_column_list)\n",
    "    \n",
    "    data_obj_train = data_df_train[obj_column_list]\n",
    "    data_obj_test = data_df_test[obj_column_list]\n",
    "    \n",
    "    encoder = DictVectorizer(sparse=False)\n",
    "    obj_train_encode = encoder.fit_transform(data_obj_train.to_dict(orient='record'))\n",
    "    obj_test_encode = encoder.transform(data_obj_test.to_dict(orient='record'))\n",
    "    \n",
    "    data_df_train = pd.concat([data_df_train, pd.DataFrame(obj_train_encode)], axis=1).drop(obj_column_list,axis=1)\n",
    "    data_df_test = pd.concat([data_df_test, pd.DataFrame(obj_test_encode)], axis=1).drop(obj_column_list,axis=1)\n",
    "    print(\"obj encode finished\")\n",
    "\n",
    "    return data_df_train, data_df_test \n",
    "\n",
    "def one_hot_encoder(data_df_train, data_df_test, obj_column_list=None, concat_first=True):\n",
    "    if not obj_column_list:\n",
    "        obj_column_list, _ = get_obj_column(data_df_train)\n",
    "    \n",
    "    print(\"obj_column_list: \", obj_column_list)\n",
    "    \n",
    "    data_obj_train = data_df_train[obj_column_list]\n",
    "    data_obj_test = data_df_test[obj_column_list]\n",
    "    \n",
    "    if concat_first:  # 将训练集和测试集进行拼接，来解决测试集中出现新特征的问题，适用于小数据量\n",
    "        len_train = data_df_train.shape[0]  # 先记录训练集数量，以备后续还原\n",
    "        data_df_obj = pd.concat([data_obj_train, data_obj_test], join=\"inner\").reset_index(drop=True)\n",
    "\n",
    "        encoder = OneHotEncoder(sparse=False)\n",
    "        total_result = encoder.fit_transform(data_df_obj)\n",
    "\n",
    "        total_result = pd.DataFrame(total_result)\n",
    "        obj_train_encode = total_result.iloc[:len_train].reset_index(drop=True)\n",
    "        obj_test_encode = total_result.iloc[len_train:].reset_index(drop=True)\n",
    "    else:  # 直接对新特征进行忽略，减少内存开销\n",
    "        encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "        obj_train_encode = encoder.fit_transform(data_obj_train)\n",
    "        obj_test_encode = encoder.transform(data_obj_test)\n",
    "    \n",
    "    data_df_train = pd.concat([data_df_train, obj_train_encode], axis=1).drop(obj_column_list,axis=1)\n",
    "    data_df_test = pd.concat([data_df_test, obj_test_encode], axis=1).drop(obj_column_list,axis=1)    \n",
    "    print(\"obj encode finished\")\n",
    "    \n",
    "    return data_df_train, data_df_test\n",
    "\n",
    "data_df_train_encode, data_df_test_encode = dict_encoder(train_data, test_data)\n",
    "# data_df_train_encode, data_df_test_encode = one_hot_encoder(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>...</th>\n",
       "      <th>823</th>\n",
       "      <th>824</th>\n",
       "      <th>825</th>\n",
       "      <th>826</th>\n",
       "      <th>827</th>\n",
       "      <th>828</th>\n",
       "      <th>829</th>\n",
       "      <th>830</th>\n",
       "      <th>831</th>\n",
       "      <th>832</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 839 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass   Age  SibSp  Parch     Fare    0    1    2    3  ...  \\\n",
       "0         0       3  22.0      1      0   7.2500  0.0  0.0  0.0  0.0  ...   \n",
       "1         1       1  38.0      1      0  71.2833  0.0  0.0  0.0  0.0  ...   \n",
       "2         1       3  26.0      0      0   7.9250  0.0  0.0  0.0  0.0  ...   \n",
       "3         1       1  35.0      1      0  53.1000  0.0  0.0  0.0  0.0  ...   \n",
       "4         0       3  35.0      0      0   8.0500  0.0  0.0  0.0  0.0  ...   \n",
       "\n",
       "   823  824  825  826  827  828  829  830  831  832  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 839 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 简单再看看当前数据形式\n",
    "data_df_train_encode.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 5、对年龄进行简单分箱操作\n",
    "# 示例：pd.cut(data_aa[\"Age\"],[0,20,30,40,70,100],labels=[\"少年\",\"青年\",\"中年\",\"老年\",\"暮年\"],right=False)\n",
    "data_df_train_encode[\"Age\"] = pd.Series(pd.cut(data_df_train_encode[\"Age\"], [0,20,30,40,70,100], labels=False, right=False))\n",
    "data_df_test_encode[\"Age\"] = pd.Series(pd.cut(data_df_test_encode[\"Age\"], [0,20,30,40,70,100], labels=False, right=False))\n",
    "\n",
    "# 6、因为还存在类如Fare这种取值过大的特征列，故而做个去量纲操作，比如标准化、归一化、区间放缩法或者小数定标规范化\n",
    "# 这里选择区间放缩，即x=(x-x.min)/(x.max-x.min)。目的是为了减少数值太大导致的易过拟合情况\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "data_df_train_encode[\"Fare\"] = min_max_scaler.fit_transform(data_df_train_encode[[\"Fare\"]])  # 可以考虑只对某些列进行放缩\n",
    "data_df_test_encode[\"Fare\"] = min_max_scaler.fit_transform(data_df_test_encode[[\"Fare\"]])\n",
    "# label_column = \"Survived\"\n",
    "# data_df_train_encode = min_max_scaler.fit_transform(data_df_train_encode.drop(label_column,axis=1))  # 也可以考虑对全部放缩\n",
    "# data_df_test_encode = min_max_scaler.transform(data_df_test_encode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 上模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1、定义得分函数，分类问题四个常见指标可以简单进行计算\n",
    "# 同时也可以配置KS值与KS曲线来辅助一同进行结果阐述\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "\n",
    "def get_scores(y_true, y_predict):\n",
    "    label_num = max(len(set(y_predict)), len(set(y_true)))\n",
    "    average = \"macro\" if label_num > 2 else \"binary\"  # 二分类用binary， 多分类用macro\n",
    "    precise = '%.4f' % precision_score(y_true, y_predict, average=average)\n",
    "    accuracy = '%.4f' % accuracy_score(y_true, y_predict)\n",
    "    recall = '%.4f' % recall_score(y_true, y_predict, average=average)\n",
    "    f1 = '%.4f' % f1_score(y_true, y_predict, average=average)\n",
    "    return [precise, accuracy, recall, f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data split finished\n"
     ]
    }
   ],
   "source": [
    "# 2、进行数据拆分\n",
    "from sklearn.model_selection import train_test_split\n",
    "label_column = \"Survived\"\n",
    "\n",
    "features, labels = data_df_train_encode.drop(label_column, axis=1), data_df_train_encode[label_column]\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.33, random_state=0, stratify=labels)\n",
    "print(\"data split finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.8200', '0.8339', '0.7257', '0.7700']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\softWare\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# 3.1、简简单单使用逻辑回归\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_ins = LogisticRegression(random_state=0, solver=\"sag\", multi_class=\"multinomial\")\n",
    "logistic_ins.fit(x_train, y_train)\n",
    "y_predict_3 = logistic_ins.predict(x_test)\n",
    "print(get_scores(y_test, y_predict_3))\n",
    "\n",
    "result = pd.DataFrame(logistic_ins.predict(data_df_test_encode))\n",
    "result.to_csv('lr_titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.7526', '0.7831', '0.6460', '0.6952']\n",
      "best param and best score are: {'min_samples_leaf': 1, 'min_samples_split': 2, 'max_depth': None}, 0.7969747899159663\n",
      "['0.7526', '0.7831', '0.6460', '0.6952']\n"
     ]
    }
   ],
   "source": [
    "# 3.2、使用CART决策树并通过网格搜索的方式进行调参\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "cart_ins = DecisionTreeClassifier(criterion=\"gini\", random_state=0)\n",
    "cart_ins.fit(x_train, y_train)\n",
    "y_predict_1 = cart_ins.predict(x_test)\n",
    "print(get_scores(y_test, y_predict_1))  \n",
    "\n",
    "# 使用网格搜索看看最佳参数设置\n",
    "cart_params = [{\"max_depth\": [None, 2, 5, 10],\n",
    "                \"min_samples_split\": [2, 3, 5],\n",
    "                \"min_samples_leaf\": [1, 2, 3]}]\n",
    "\n",
    "grid_ins = GridSearchCV(cart_ins, cart_params, cv=5)\n",
    "grid_ins.fit(x_train, y_train)\n",
    "print(\"best param and best score are: {}, {}\".format(grid_ins.best_params_, grid_ins.best_score_))\n",
    "\n",
    "y_predict_2 = grid_ins.predict(x_test)\n",
    "print(get_scores(y_test, y_predict_2))  \n",
    "\n",
    "result = pd.DataFrame(grid_ins.predict(data_df_test_encode))\n",
    "result.to_csv('cart_grid_search_titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\softWare\\Anaconda\\lib\\site-packages\\xgboost\\__init__.py:29: FutureWarning: Python 3.5 support is deprecated; XGBoost will require Python 3.6+ in the near future. Consider upgrading to Python 3.6+.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.8171008403361343\n",
      "Generation 2 - Current best internal CV score: 0.8171008403361343\n",
      "Generation 3 - Current best internal CV score: 0.8171008403361343\n",
      "Generation 4 - Current best internal CV score: 0.8187675070028011\n",
      "Generation 5 - Current best internal CV score: 0.8221288515406162\n",
      "Best pipeline: LinearSVC(RobustScaler(LinearSVC(input_matrix, C=15.0, dual=False, loss=squared_hinge, penalty=l1, tol=0.01)), C=10.0, dual=False, loss=squared_hinge, penalty=l1, tol=1e-05)\n",
      "0.8203389830508474\n"
     ]
    }
   ],
   "source": [
    "# 3.3、必杀器：使用自动学习来寻找最厉害的模型及其超参\n",
    "from tpot import TPOTClassifier\n",
    "tpot = TPOTClassifier(generations=5, population_size=20, verbosity=2)\n",
    "tpot.fit(x_train, y_train)\n",
    "print(tpot.score(x_test, y_test))\n",
    "tpot.export('titanic_tpot_pipeline.py')\n",
    "result = pd.DataFrame(tpot.predict(data_df_test_encode))\n",
    "result.to_csv(\"automl_titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
